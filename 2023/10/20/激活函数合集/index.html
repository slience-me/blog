<!DOCTYPE html> <html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" /> <title>机器学习｜【机器学习合集】激活函数合集 &mdash; Slience_me的博客</title> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/vendor/primer-css/css/primer.css"> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/css/components/collection.css"> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/css/components/repo-card.css"> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/css/sections/repo-list.css"> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/css/components/boxed-group.css"> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/css/globals/common.css"> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/css/globals/responsive.css"> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/css/posts/index.css"> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/vendor/octicons/octicons/octicons.css"> <link rel="stylesheet" href="https://mazhuang.org/rouge-themes/dist/github.css"> <link rel="stylesheet" href="https://blog.slienceme.cn/assets/vendor/share.js/dist/css/share.min.css"> <link rel="canonical" href="https://blog.slienceme.cn/2023/10/20/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/"> <link rel="alternate" type="application/atom+xml" title="Slience_me的博客" href="https://blog.slienceme.cn/feed.xml"> <link rel="shortcut icon" href="https://blog.slienceme.cn/favicon.ico"> <meta property="og:title" content="机器学习｜【机器学习合集】激活函数合集"> <meta name="keywords" content="机器学习"> <meta name="og:keywords" content="机器学习"> <meta name="description" content=""> <meta name="og:description" content=""> <meta property="og:url" content="https://blog.slienceme.cn/2023/10/20/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/"> <meta property="og:site_name" content="Slience_me的博客"> <meta property="og:type" content="article"> <meta property="og:locale" content="zh_CN" /> <meta property="article:published_time" content="2023-10-20"> <meta name="google-site-verification" content="2feHjT1GNs1Yi2JQfOtdYx7d048naG_-cMwZaDAopIA" /> <script src="https://blog.slienceme.cn/assets/vendor/jquery/dist/jquery.min.js"></script> <script src="https://blog.slienceme.cn/assets/js/main.js"></script> </head> <body class="" data-mz=""> <header class="site-header"> <div class="container"> <h1><a href="https://blog.slienceme.cn/" title="Slience_me的博客"><span class="octicon octicon-mark-github"></span> Slience_me的博客</a></h1> <button class="collapsed mobile-visible" type="button" onclick="toggleMenu();"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <nav class="site-header-nav" role="navigation"> <a href="https://blog.slienceme.cn/" class="site-header-nav-item" target="" title="首页">首页</a> <a href="https://blog.slienceme.cn/categories/" class="site-header-nav-item" target="" title="分类">分类</a> <a href="https://blog.slienceme.cn/archives/" class="mobile-hidden site-header-nav-item" target="" title="归档">归档</a> <a href="https://blog.slienceme.cn/open-source/" class="mobile-hidden site-header-nav-item" target="" title="开源">开源</a> <a href="https://blog.slienceme.cn/fragments/" class="site-header-nav-item" target="" title="片段">片段</a> <a href="https://blog.slienceme.cn/wiki/" class="site-header-nav-item" target="" title="维基">维基</a> <a href="https://blog.slienceme.cn/links/" class="mobile-hidden site-header-nav-item" target="" title="链接">链接</a> <a href="https://blog.slienceme.cn/about/" class="site-header-nav-item" target="" title="关于">关于</a> <a class="mobile-hidden" href="https://blog.slienceme.cn/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a> </nav> </div> </header> <section class="collection-head small geopattern" data-pattern-id="机器学习｜【机器学习合集】激活"> <div class="container"> <div class="columns"> <div class="column three-fourths"> <div class="collection-title"> <h1 class="collection-header">机器学习｜【机器学习合集】激活函数合集</h1> <div class="collection-info"> <span class="meta-info"> <span class="octicon octicon-calendar"></span> 2023/10/20 </span> <span class="meta-info"> <span class="octicon octicon-file-directory"></span> <a href="https://blog.slienceme.cn/categories/#机器学习" title="机器学习">机器学习</a> </span> <span class="meta-info"> <span class="octicon octicon-clock"></span> 共 4065 字，约 12 分钟 </span> </div> </div> </div> <div class="column one-fourth mobile-hidden"> <div class="collection-title"> </div> </div> </div> </div> </section> <section class="container content"> <div class="columns"> <div class="column three-fourths" > <article class="article-content markdown-body"> <p><img src="https://blog.slienceme.cn/images/posts/logo_slienceme3.png" alt="img" /></p> <p>本文作者： <a href="https://slienceme.cn/">slience_me</a></p> <hr /> <h3 id="综述">综述</h3> <blockquote> <p>这些都是神经网络中常用的激活函数，它们在非线性变换方面有不同的特点。以下是这些激活函数的主要区别： 1. <code class="language-plaintext highlighter-rouge">Sigmoid和Tanh激活函数</code>： - Sigmoid函数和Tanh函数都是S型函数，将输入映射到（0，1）或（-1，1）范围内。它们在输入远离零时会饱和，导致梯度消失问题。 - Sigmoid函数输出范围是（0，1），而Tanh函数输出范围是（-1，1）。 - 在深度神经网络中，它们往往不如ReLU等其他激活函数表现出色。 2. <code class="language-plaintext highlighter-rouge">ReLU激活函数（Rectified Linear Unit）</code>： - ReLU是分段线性函数，当输入为正时输出等于输入，而当输入为负时输出为零。 - ReLU激活函数在训练深度神经网络时通常表现良好，因为它不会导致梯度消失问题，并且计算速度快。 - 然而，它也有一些问题，如死亡神经元问题，即某些神经元在训练中可能永远不会激活。 3. <code class="language-plaintext highlighter-rouge">ReLU激活函数的改进（Leaky ReLU、Parametric ReLU、Exponential Linear Unit等）</code>： - 这些是ReLU的改进版本，旨在解决死亡神经元问题。它们允许小的负输入值通过，从而改善了ReLU的性能。 4. <code class="language-plaintext highlighter-rouge">近似ReLU激活函数（Swish、Mish等）</code>： - 近似ReLU函数试图在ReLu的非线性性和平滑性之间取得平衡。Swish和Mish是其中的两个例子，它们在输入大于零时光滑且非线性，但在输入小于零时也具有一定的非线性性。 - Swish是x乘以S型函数，而Mish是x与双曲正切函数的乘积。这些函数在某些情况下可以表现出色，但并不总是适用于所有情况。 5. <code class="language-plaintext highlighter-rouge">Maxout激活函数</code>： - Maxout是一种非线性激活函数，它在每个神经元的输入中选择最大的那个。它的输出是线性的片段中的最大值。 - Maxout激活函数允许网络自行学习不同的线性片段，从而提高网络的表达能力。 6. <code class="language-plaintext highlighter-rouge">自动搜索的激活函数（Swish等）</code>： - Swish是通过自动搜索和优化得到的激活函数。它的设计基于一定的数学原理和实验结果，旨在提高神经网络的性能。 - 这些自动搜索得到的激活函数通常经过大量实验验证，以确保它们在各种任务中表现良好。</p> </blockquote> <blockquote> <ul> <li> <h2 id="每种激活函数都有其适用的场景和优点选择哪种激活函数通常取决于具体的任务和网络结构在深度学习中通常需要进行实验来确定哪种激活函数在特定情况下效果最好">每种激活函数都有其适用的场景和优点，选择哪种激活函数通常取决于具体的任务和网络结构。在深度学习中，通常需要进行实验来确定哪种激活函数在特定情况下效果最好</h2> <h3 id="1-s激活函数sigmoidtanh">1. S激活函数(sigmoid&amp;Tanh)</h3> </li> </ul> </blockquote> <blockquote> <ul> <li> <p>Sigmoid函数在机器学习中经常用作激活函数，但它在某些情况下容易出现梯度消失问题，这是因为它的特性导致了梯度在饱和区域非常接近于零。</p> </li> <li> <p>Sigmoid函数的数学表达式如下： S(x) = 1 / (1 + e^(-x))</p> </li> </ul> </blockquote> <blockquote> <ul> <li>当输入x接近正无穷大（x → +∞）时，Sigmoid函数的输出趋近于1，而当输入x接近负无穷大（x → -∞）时，输出趋近于0。这意味着Sigmoid函数具有饱和性质，即在这些极端值附近，它的梯度接近于零。这就是梯度消失问题的根本原因。</li> </ul> </blockquote> <blockquote> <ul> <li>当你使用Sigmoid激活函数时，如果输入数据的绝对值非常大，梯度接近于零，这会导致反向传播算法中的梯度变得非常小，从而权重更新几乎不会发生，导致训练变得非常缓慢或根本无法进行有效的学习。这尤其在深度神经网络中更加明显，因为梯度会以指数方式递减，这就是为什么Sigmoid函数在深度神经网络中容易出现梯度消失问题。</li> </ul> </blockquote> <blockquote> <ul> <li>为了克服这个问题，人们开始使用其他激活函数，如ReLU（Rectified Linear Unit）和其变种，它们不具有Sigmoid函数的饱和性质，因此在训练深度神经网络时更加稳定。ReLU激活函数的导数在正区域始终为1，因此梯度不会在正区域消失。这有助于更有效地进行梯度传播和权重更新，减少了梯度消失问题。</li> </ul> </blockquote> <p><img src="https://blog.slienceme.cn/images/posts/fdafd1e3f3c64052b381edf385d01dc3.png" alt="Alt Text" /> <img src="https://blog.slienceme.cn/images/posts/fc8c3e931f574ad899d87fe57957f4de.jpeg" alt="Alt Text" /> <img src="https://blog.slienceme.cn/images/posts/fb65aedcd966403da4ff7c7820ef8012.png" alt="Alt Text" /> <img src="https://blog.slienceme.cn/images/posts/b74b4c1d260f4da68e5cabf28b6f3b60.png" alt="Alt Text" /></p> <blockquote> <ul> <li>Tanh（双曲正切）函数在某些情况下也可能出现梯度消失问题，尽管它相对于Sigmoid函数有一些改进，但仍然具有饱和性质，导致梯度在饱和区域接近于零。</li> <li>Tanh函数的数学表达式如下： Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))</li> </ul> </blockquote> <blockquote> <ul> <li>Tanh函数的输出范围在-1到1之间，当输入x接近正无穷大时，它的输出趋近于1，当输入x接近负无穷大时，它的输出趋近于-1。这就意味着Tanh函数在极端值附近也具有饱和性质，梯度接近于零。</li> </ul> </blockquote> <blockquote> <ul> <li>梯度消失问题发生的原因在于反向传播算法中的链式法则，其中导数相乘。当使用Tanh函数时，如果在网络的前向传播过程中输出值位于饱和区域，梯度将变得非常小，反向传播中的梯度也会随之减小。这会导致权重更新非常缓慢，尤其是在深度神经网络中。</li> </ul> </blockquote> <blockquote> <ul> <li>虽然Tanh函数相对于Sigmoid函数在某些情况下更好，因为它的输出范围在-1到1之间，但在解决梯度消失问题方面，它仍然不如一些其他激活函数，如ReLU（Rectified Linear Unit）及其变种。ReLU在正区域具有恒定梯度，因此不容易出现梯度消失问题。为了克服梯度消失问题，深度神经网络中的一种常见做法是使用ReLU或其变种，同时采用一些正则化技术和初始化策略来稳定训练过程。</li> </ul> </blockquote> <h3 id="2-relu激活函数">2. ReLU激活函数</h3> <blockquote> <ul> <li>ReLU（Rectified Linear Unit）是一种常用的激活函数，它在输入大于零时输出输入值，而在输入小于或等于零时输出零。这意味着ReLU是非零中心化的，因为它的输出的均值（平均值）不是零，而是正的。这与一些其他激活函数，如tanh和Sigmoid不同，它们的输出均值通常接近于零。</li> <li>为什么ReLU是非零中心化的并且没有负激活值，可以归结为其定义方式。ReLU函数的数学表达式如下： f(x) = max(0, x)</li> </ul> </blockquote> <blockquote> <ul> <li>在这个函数中，当输入x大于零时，它输出x，而当输入x小于等于零时，输出零。这意味着ReLU在正区域（x&gt;0）内有激活值，但在负区域（x&lt;=0）内没有激活值。因为ReLU截断了负值，所以其均值是正的。</li> </ul> </blockquote> <blockquote> <ul> <li>这种非零中心化的性质有一些影响： <ol> <li>梯度消失问题缓解：与tanh和Sigmoid等激活函数不同，ReLU在正区域的梯度始终为1，这有助于减轻梯度消失问题，因为梯度不会在正区域消失。</li> <li>稀疏激活性：由于ReLU在负区域没有激活值，神经元可以学习选择性地激活，这有助于网络的稀疏表示，这意味着每个神经元仅在特定情况下激活，而其他时候保持静止，这对于特征选择和表示学习很有用。</li> </ol> </li> </ul> </blockquote> <blockquote> <ul> <li>尽管ReLU有许多优点，但它也有一些问题，例如死亡神经元问题，其中某些神经元在训练中永远保持非活跃状态。为了克服这些问题，人们发展了一些ReLU的变种，如Leaky ReLU和Parametric ReLU（PReLU），它们允许小的负输入值通过，从而改善了ReLU的性能。这些变种可以使神经网络更容易训练。</li> </ul> <p><img src="https://blog.slienceme.cn/images/posts/05ebde98310044f38c9d0c6bc339d738.png" alt="Alt Text" /></p> <h3 id="3-relu激活函数的改进">3. ReLU激活函数的改进</h3> <p><img src="https://blog.slienceme.cn/images/posts/f9baed834d2140f38dbfe9df67c84119.png" alt="Alt Text" /> <img src="https://blog.slienceme.cn/images/posts/0d06ed8711f04f9a9610fc714c349efc.png" alt="Alt Text" /></p> <h3 id="4-近似relu激活函数">4. 近似ReLU激活函数</h3> <p><img src="https://blog.slienceme.cn/images/posts/ffc49914a79a4e6c932fd10f6244dcae.png" alt="Alt Text" /></p> <h3 id="5-maxout激活函数">5. Maxout激活函数</h3> </blockquote> <blockquote> <ul> <li>Maxout是一种激活函数，它在深度学习中用于神经网络的非线性变换。与传统的激活函数如ReLU、Sigmoid和tanh不同，Maxout具有独特的结构，它的主要特点是取输入的最大值，因此可以视为线性片段的极大化。以下是Maxout激活函数的定义：</li> <li>对于Maxout激活函数，给定多个线性组合的输入，它将这些线性组合中的最大值作为输出。具体来说，考虑两个线性组合： Z1 = w1<em>x + b1 Z2 = w2</em>x + b2</li> <li>Maxout激活函数输出的值为：Maxout(x) = max(Z1, Z2)</li> </ul> </blockquote> <blockquote> <ul> <li>Maxout的主要特点和优点包括： <ol> <li>非线性性质：Maxout函数是一种非线性激活函数，因为它取输入中的最大值，从而引入了非线性性质，使神经网络能够学习更复杂的函数。</li> <li>灵活性：Maxout允许神经网络学习不同的线性片段，而不受限于单一的线性关系。这可以增加模型的表达能力，有助于处理各种数据分布和特征。</li> <li>抗噪声性：Maxout激活函数在一定程度上对噪声具有抗性，因为它取输入中的最大值，可以消除一些不必要的噪声信号。</li> <li>降低过拟合风险：Maxout具有更多的参数，允许网络在训练中拟合更多的数据，从而降低了过拟合的风险。</li> </ol> </li> </ul> </blockquote> <blockquote> <ul> <li>尽管Maxout在理论上具有一些优势，但在实际应用中，它并不像ReLU那样常见。这是因为Maxout的参数数量较多，可能需要更多的数据和计算资源来训练。此外，ReLU和其变种在实践中通常表现得非常出色，因此它们更常见。然而，Maxout仍然是一个有趣的激活函数，特别适用于特定的深度学习任务和研究领域。</li> </ul> </blockquote> <p><img src="https://blog.slienceme.cn/images/posts/9397fb020eae4828a559c512247f11a0.png" alt="Alt Text" /></p> <h3 id="6-自动搜索的激活函数swish">6. 自动搜索的激活函数Swish</h3> <blockquote> <ul> <li>Swish是一种激活函数，最初由Google研究员在2017年提出。Swish函数的定义如下：Swish(x) = x * sigmoid(x)</li> <li>其中，x是输入，sigmoid(x)表示x经过S型函数（Sigmoid函数）的输出。Swish函数是一种非线性激活函数，它在一定程度上结合了线性和非线性的特性。</li> </ul> </blockquote> <blockquote> <ul> <li>Swish函数的特点和优势包括： 1. 平滑性：Swish函数是平滑的，与ReLU等分段线性函数相比，它在激活值的变化上更加平滑。这有助于梯度的更加连续传播，有助于训练深度神经网络。 2. 非线性性质：Swish在Sigmoid函数的基础上引入了非线性，这使得它能够捕捉更复杂的数据模式，使神经网络更具表达能力。 3. 渐进性：与ReLU不同，Swish函数在输入趋于正无穷大时不会饱和，而是渐进地接近于线性函数x。这意味着Swish函数在正值区域仍然具有一定的非线性性质，从而有助于避免一些梯度消失问题。 4. 可学习性：Swish函数是可学习的，它的参数（例如，Sigmoid函数的斜率）可以通过反向传播算法进行调整，以适应特定任务和数据分布。</li> </ul> </blockquote> <blockquote> <ul> <li>尽管Swish在理论上有一些优势，但在实践中，它的性能通常介于ReLU和Sigmoid之间。因此，选择使用Swish还是其他激活函数取决于具体的任务和实验。有时，Swish可能对某些问题效果很好，但对于其他问题，标准的ReLU或其变种仍然是首选。在深度学习中，激活函数通常是可以调整的超参数，因此可以进行实验来选择最适合特定任务的激活函数。</li> </ul> <p><img src="https://blog.slienceme.cn/images/posts/783cb568c3c643cbbda655af6cdc22d6.png" alt="Alt Text" /></p> </blockquote> <p><strong>注意：部分内容来自 阿里云天池</strong></p> <div style="margin-top:2em;padding:0 1.5em;border:1px solid #d3d3d3;background-color:#deebf7"> <h3>文档信息</h3> <ul> <li>本文作者：<a href="https://blog.slienceme.cn" target="_blank">slience_me</a></li> <li>本文链接：<a href="https://blog.slienceme.cn/2023/10/20/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/" target="_blank">https://blog.slienceme.cn/2023/10/20/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/</a></li> <li>版权声明：自由转载-非商用-非衍生-保持署名（<a href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank">创意共享3.0许可证</a>）</li> </ul> </div> </article> <div class="share"> <div class="share-component" data-disabled='qq,facebook'></div> </div> <div class="comment"> <script src="https://giscus.app/client.js" data-repo="slience-me/blog-comments" data-repo-id="R_kgDOKr27jA" data-category="Announcements" data-category-id="DIC_kwDOKr27jM4CnWMe" data-mapping="title" data-strict="1" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous" async> </script> </div> </div> <div class="column one-fourth"> <h3>Search</h3> <div id="site_search"> <input style="width:96%" type="text" id="search_box" placeholder="Search"> </div> <ul id="search_results" style="font-size:14px;list-style-type:none;padding-top:10px;padding-left:10px;"></ul> <script src="https://blog.slienceme.cn/assets/js/simple-jekyll-search.min.js"></script> <script type="text/javascript"> SimpleJekyllSearch({ searchInput: document.getElementById('search_box'), resultsContainer: document.getElementById('search_results'), json: 'https://blog.slienceme.cn/assets/search_data.json?v=1740598838', searchResultTemplate: '<li><a href="{url}" title="{title}">{title}</a></li>', noResultsText: 'No results found', limit: 10, fuzzy: false, exclude: ['Welcome'] }) </script> <h3 class="post-directory-title mobile-hidden">Table of Contents</h3> <div id="post-directory-module" class="mobile-hidden"> <section class="post-directory"> <dl></dl> </section> </div> <script src="https://blog.slienceme.cn/assets/js/jquery.toc.js"></script> </div> </div> </section> <footer class="container"> <div class="site-footer" role="contentinfo"> <div class="copyright left mobile-block"> Copyright@ 2019-2025 slience_me 版权所有&emsp; <img src="/images/logo/logo.png" class="w-full" style="width: 12px;"> <a href="https://beian.mps.gov.cn/#/query/webSearch?code=13102202000626" rel="noreferrer" target="_blank">冀公网安备13102202000626</a>&emsp; <a href="https://beian.miit.gov.cn/#/Integrated/index" target="_blank" rel="noreferrer">津ICP备2024026565号-1</a> <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a> </div> <ul class="site-footer-links right mobile-hidden"> <li> <a href="javascript:window.scrollTo(0,0)" >TOP</a> </li> </ul> <br/> <script defer src="https://vercount.one/js"></script> <div class="mobile-hidden" style="margin-top:8px"> <span id="busuanzi_container_site_pv" style="display:none"> 本站访问量<span id="busuanzi_value_site_pv"></span>次 </span> <span id="busuanzi_container_site_uv" style="display:none"> / 本站访客数<span id="busuanzi_value_site_uv"></span>人 </span> <span id="busuanzi_container_page_pv" style="display:none"> / 本页访问量<span id="busuanzi_value_page_pv"></span>次 / 统计始于2024-12-01 </span> </div> </div> </footer> <div class="tools-wrapper"> <a class="gotop" href="#" title="回到顶部"><span class="octicon octicon-arrow-up"></span></a> </div> <script src="https://blog.slienceme.cn/assets/vendor/share.js/dist/js/share.min.js"></script> <script src="https://blog.slienceme.cn/assets/js/geopattern.js"></script> <script> jQuery(document).ready(function($) { $('.geopattern').each(function(){ $(this).geopattern($(this).data('pattern-id')); }); /* hljs.initHighlightingOnLoad(); */ }); </script> </body> </html>
