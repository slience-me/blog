---
layout: post
title: 机器学习｜【机器学习合集】模型设计之残差网络
categories: [机器学习]
description: 【机器学习合集】模型设计之残差网络
keywords: 机器学习
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---

![img](https://raw.githubusercontent.com/slience-me/picGo/master/images/logo_slienceme3.jpeg)

本文作者： [slience_me](https://slienceme.cn/)

---

# 模型设计之残差网络

> - 残差网络（Residual Network，通常缩写为ResNet）是一种深度神经网络架构，最早由微软研究员提出。ResNet的核心思想是通过引入残差块（Residual Blocks）来解决深度神经网络训练中的梯度消失和梯度爆炸问题，从而使得更深的网络能够更容易地训练和优化。
> 以下是ResNet的主要特点和设计原则：

> 1. `残差块（Residual Blocks）`：
>    - 残差块是ResNet的基本构建单元，它包含了跳跃连接（skip connection）和残差映射（residual mapping）。
>    - 跳跃连接允许信息在不同层之间直接传递，而不是通过多个非线性激活函数。这有助于避免梯度消失问题，使网络更容易训练。
>    - 残差映射通过跳跃连接将输入特征与经过一些卷积层变换的特征相加，从而使网络学习残差（即差异），而不是完整的映射。这有助于网络捕捉更丰富的特征。
> 2. `深度增加`：
>    - ResNet允许非常深的网络，通常包含数十到数百个层。通过使用残差块，可以轻松增加网络的深度而不会导致性能下降。
>    - 更深的网络有助于学习更复杂的特征，提高了模型的表现，特别适用于大规模图像分类等任务。
> 3. `瓶颈结构`：
>    - 为了减少参数量和计算复杂度，ResNet引入了瓶颈结构，其中每个残差块包含了一个较小的1x1卷积层、一个3x3卷积层和另一个1x1卷积层。这可以有效减少通道数和计算开销。
> 4. `预训练和微调`：
>    - ResNet模型通常通过在大规模图像数据集上进行预训练，然后微调到特定任务。这种迁移学习方法能够在数据有限的情况下取得很好的结果。
> 5. `引入注意力机制`：
>    - 可以将注意力机制引入ResNet以增强其性能，特别是在处理复杂的计算机视觉任务中。通过引入注意力机制，模型可以更好地关注重要的特征。

>  - 总之，ResNet是一种强大的深度学习模型架构，已被广泛应用于计算机视觉任务，如图像分类、目标检测和图像分割。其核心思想是通过残差块来构建深层神经网络，从而克服深度神经网络中的梯度问题，实现更好的性能。

## 1. 什么是残差结构
### 1.1 网络加深遇到的优化问题
- <font color=#E91E63>**网络达到一定深度后，梯度消散与爆炸带来的性能下降问题**</font>
- 此前的解决方案：更好的优化方法，更好的初始化策略，BN层，ReLU激活函数等
![Alt Text](/images/posts/1546096b22704d6a86cd426635e75233.png)
### 1.2 short connect技术

- <font color=#E91E63>**在信号处理系统中，对输入数据进行中心化转换，即将数据减去均值，被广泛验证有利于加快系统的学习速度。**</font>
![Alt Text](/images/posts/0c2ecbd65b3e48ab923cfb0d9d06ee84.png)


**早期验证**

- <font color=#E91E63>**2012年Tapani Raiko验证了shortcut connections和非线性变换提高了随机梯度下降算法的学习能力，并且提高了模型的泛化能力。**</font>
![Alt Text](/images/posts/5738e2d68616410f99f8d32ba157e6fd.png)

- <font color=#E91E63>**2015年Rupesh Kumar Srivastava提出highway network(残差的结构)，借鉴了来自于LSTM的控制门的思想**</font>
![Alt Text](/images/posts/12a17dd0e21c4796bb774c3f470ee5c6.png)
## 2. 残差网络及有效性理解
### 2.1 残差网络

- <font color=#E91E63>**2015年何凯明等人提出deep residual network**</font>
![Alt Text](/images/posts/8571d473fea84f4892adfe6c582443d4.png)

- <font color=#E91E63>**缓解梯度消失问题，训练上1000层的模型**</font>
![Alt Text](/images/posts/668c8c0a74f542d9bc619a9b605d1342.png)
- F的梯度变化远远大于G，引入残差后的映射对输出的变化更敏感，这样有利于网络参数进行学习

- 神经网络的退化(只有少量的隐藏单元对不同的输入改变它们的激活值)是难以训练深层网络根本原因所在，残差打破了网络的对称性，消除了网络中的奇点
![Alt Text](/images/posts/85bd5e64ac0f4883994858fda6900f92.png)

- <font color=#E91E63>**残差网络可以看作是多个不同深度模型的集成，提高了泛化能力，”Residual networks behave like ensembles of relatively shallow networks“**</font>
![Alt Text](/images/posts/f95ffda7a37f4c9eabc1ed73b254aca7.png)
## 3. 残差网络的发展

### 3.1 密集残差网络

- <font color=#E91E63>**提高多层通道的利用率，密集连接网络(DenseNet)，增强各层的信息流动**</font>
![Alt Text](/images/posts/985d5e8fc2564f6c8978909b9ddd7569.png)
### 3.2 更宽的残差网络(wide resnet)

- <font color=#E91E63>**通道数更大**</font>
![Alt Text](/images/posts/43b9202147db4b76823c2b766aeaac6c.png)
### 3.3 分组残差网络

- <font color=#E91E63>**分组成若干个相同的子分支**</font>
![Alt Text](/images/posts/cc9da6b9dcb34f2a83475ae5edee4509.png)
### 3.4 Dual Path Network

- <font color=#E91E63>**ResNext与DenseNet的结合**</font>
![Alt Text](/images/posts/59fef9e624f645e2b4dbc751da41058b.png)
### 3.5 加权残差网络

- <font color=#E91E63>**残差网络中两个通道之间相加时一条已经激活，另一条没有，提出将激活函数提前到残差通道，然后进行加权融合的思路**</font>
![Alt Text](/images/posts/847e1bdca7174e64af0b34c335f3757c.png)
### 3.6 预激活残差

- <font color=#E91E63>**改变卷积+归一化+激活函数(conv+bn+relu)的顺序**</font>
![Alt Text](/images/posts/da17001462514d7586d66f37f7c45cdc.png)

**注意：部分内容来自阿里云天池**
